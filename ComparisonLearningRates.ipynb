{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec7146ef-c16d-4196-8ba4-5fb0b3d6c7a4",
   "metadata": {},
   "source": [
    "# Assignment Two\n",
    "# Sign-Based Adaptive Learning Rate for Neural Network Galaxy Classification\n",
    "## Name: Wadalisa O. Molokwe\n",
    "### Student Number: u19258349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d61a897-cb93-416c-a56f-35b033bb02e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit CPU\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55554fde-4901-45c6-af29-1c6ba8df2331",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24e69630-dc31-42f8-9a6e-d057c06f0b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Reading in Data----------------------------------------------------------------------\n",
      "\n",
      "------------------------------------- First Two Instances---------------------------------------------------\n",
      "                 objid            specobjid         ra       dec         u  \\\n",
      "0  1237646587710669400  8175185722644649984  82.038679  0.847177  21.73818   \n",
      "1  1237646588247540577  8175186822156277760  82.138894  1.063072  20.66761   \n",
      "\n",
      "          g         r         i         z  modelFlux_u  ...  psfMag_z  \\\n",
      "0  20.26633  19.32409  18.64037  18.23833     2.007378  ...  19.43575   \n",
      "1  19.32016  18.67888  18.24693  18.04122     5.403369  ...  18.85012   \n",
      "\n",
      "    expAB_u   expAB_g   expAB_r   expAB_i   expAB_z   class     subclass  \\\n",
      "0  0.099951  0.311864  0.289370  0.270588  0.187182  GALAXY  STARFORMING   \n",
      "1  0.366549  0.516876  0.517447  0.552297  0.636966  GALAXY  STARFORMING   \n",
      "\n",
      "   redshift  redshift_err  \n",
      "0  0.067749      0.000015  \n",
      "1  0.105118      0.000010  \n",
      "\n",
      "[2 rows x 43 columns]\n",
      "\n",
      "----------------------------------Dataset Information ---------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 43 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   objid         100000 non-null  int64  \n",
      " 1   specobjid     100000 non-null  uint64 \n",
      " 2   ra            100000 non-null  float64\n",
      " 3   dec           100000 non-null  float64\n",
      " 4   u             100000 non-null  float64\n",
      " 5   g             100000 non-null  float64\n",
      " 6   r             100000 non-null  float64\n",
      " 7   i             100000 non-null  float64\n",
      " 8   z             100000 non-null  float64\n",
      " 9   modelFlux_u   100000 non-null  float64\n",
      " 10  modelFlux_g   100000 non-null  float64\n",
      " 11  modelFlux_r   100000 non-null  float64\n",
      " 12  modelFlux_i   100000 non-null  float64\n",
      " 13  modelFlux_z   100000 non-null  float64\n",
      " 14  petroRad_u    100000 non-null  float64\n",
      " 15  petroRad_g    100000 non-null  float64\n",
      " 16  petroRad_i    100000 non-null  float64\n",
      " 17  petroRad_r    100000 non-null  float64\n",
      " 18  petroRad_z    100000 non-null  float64\n",
      " 19  petroFlux_u   100000 non-null  float64\n",
      " 20  petroFlux_g   100000 non-null  float64\n",
      " 21  petroFlux_i   100000 non-null  float64\n",
      " 22  petroFlux_r   100000 non-null  float64\n",
      " 23  petroFlux_z   100000 non-null  float64\n",
      " 24  petroR50_u    100000 non-null  float64\n",
      " 25  petroR50_g    100000 non-null  float64\n",
      " 26  petroR50_i    100000 non-null  float64\n",
      " 27  petroR50_r    100000 non-null  float64\n",
      " 28  petroR50_z    100000 non-null  float64\n",
      " 29  psfMag_u      100000 non-null  float64\n",
      " 30  psfMag_r      100000 non-null  float64\n",
      " 31  psfMag_g      100000 non-null  float64\n",
      " 32  psfMag_i      100000 non-null  float64\n",
      " 33  psfMag_z      100000 non-null  float64\n",
      " 34  expAB_u       100000 non-null  float64\n",
      " 35  expAB_g       100000 non-null  float64\n",
      " 36  expAB_r       100000 non-null  float64\n",
      " 37  expAB_i       100000 non-null  float64\n",
      " 38  expAB_z       100000 non-null  float64\n",
      " 39  class         100000 non-null  object \n",
      " 40  subclass      100000 non-null  object \n",
      " 41  redshift      100000 non-null  float64\n",
      " 42  redshift_err  100000 non-null  float64\n",
      "dtypes: float64(39), int64(1), object(2), uint64(1)\n",
      "memory usage: 32.8+ MB\n",
      "\n",
      "----------------------------------------------- Descriptive Statistics ----------------------------------------------\n",
      "              objid     specobjid             ra            dec  \\\n",
      "count  1.000000e+05  1.000000e+05  100000.000000  100000.000000   \n",
      "mean   1.237659e+18  2.303595e+18     180.577802      23.472475   \n",
      "std    6.103756e+12  2.531359e+18      75.751994      21.140744   \n",
      "min    1.237646e+18  2.994897e+17       0.008745     -11.244273   \n",
      "25%    1.237655e+18  8.130687e+17     138.741880       3.120118   \n",
      "50%    1.237659e+18  1.457564e+18     181.492972      20.913596   \n",
      "75%    1.237663e+18  2.367902e+18     223.851863      42.259965   \n",
      "max    1.237681e+18  1.412691e+19     359.997922      68.695258   \n",
      "\n",
      "                   u              g              r              i  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean       18.518622      17.258221      16.821739      16.362611   \n",
      "std       105.082004     105.069066      95.035474     100.171155   \n",
      "min     -9999.000000   -9999.000000   -9999.000000   -9999.000000   \n",
      "25%        18.762215      17.505868      16.898845      16.527097   \n",
      "50%        19.349715      18.072640      17.459080      17.091385   \n",
      "75%        20.079470      18.656182      17.926918      17.592650   \n",
      "max        30.960000      30.420980      31.173560      30.562360   \n",
      "\n",
      "                   z    modelFlux_u  ...       psfMag_g       psfMag_i  \\\n",
      "count  100000.000000  100000.000000  ...  100000.000000  100000.000000   \n",
      "mean       15.850865      30.683321  ...      18.834259      18.020203   \n",
      "std       114.206165      76.552859  ...     105.079620     100.181687   \n",
      "min     -9999.000000     -47.451720  ...   -9999.000000   -9999.000000   \n",
      "25%        16.281327       9.288132  ...      19.257783      18.295627   \n",
      "50%        16.861105      18.195690  ...      19.763915      18.845780   \n",
      "75%        17.453848      31.259628  ...      20.408775      19.586577   \n",
      "max        28.553240    7915.306000  ...      26.174400      25.966680   \n",
      "\n",
      "            psfMag_z        expAB_u        expAB_g        expAB_r  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean       17.435735      -0.603667      -0.522111      -0.309462   \n",
      "std       114.218604     104.870665     104.871474      94.860919   \n",
      "min     -9999.000000   -9999.000000   -9999.000000   -9999.000000   \n",
      "25%        17.991602       0.299999       0.398705       0.418789   \n",
      "50%        18.563315       0.508688       0.588335       0.604795   \n",
      "75%        19.299430       0.699907       0.768804       0.773924   \n",
      "max        27.043280       1.000000       1.000000       0.999999   \n",
      "\n",
      "             expAB_i        expAB_z       redshift   redshift_err  \n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000  \n",
      "mean       -0.410153      -0.740964       0.116753       0.000179  \n",
      "std        99.991654     114.005927       0.100169       0.052189  \n",
      "min     -9999.000000   -9999.000000      -0.000833       0.000002  \n",
      "25%         0.418656       0.381288       0.055836       0.000008  \n",
      "50%         0.604254       0.575397       0.085850       0.000011  \n",
      "75%         0.773119       0.752311       0.135148       0.000015  \n",
      "max         1.000000       0.999998       0.572899      16.503710  \n",
      "\n",
      "[8 rows x 41 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mdescribe())\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# the dataset is enough to just drop values and duplicates\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\frame.py:6415\u001b[0m, in \u001b[0;36mDataFrame.dropna\u001b[1;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6412\u001b[0m     mask \u001b[38;5;241m=\u001b[39m count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m thresh\n\u001b[0;32m   6413\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   6414\u001b[0m     \u001b[38;5;66;03m# faster equivalent to 'agg_obj.count(agg_axis) == self.shape[agg_axis]'\u001b[39;00m\n\u001b[1;32m-> 6415\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mnotna\u001b[49m\u001b[43m(\u001b[49m\u001b[43magg_obj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mall(axis\u001b[38;5;241m=\u001b[39magg_axis, bool_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   6416\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   6417\u001b[0m     \u001b[38;5;66;03m# faster equivalent to 'agg_obj.count(agg_axis) > 0'\u001b[39;00m\n\u001b[0;32m   6418\u001b[0m     mask \u001b[38;5;241m=\u001b[39m notna(agg_obj)\u001b[38;5;241m.\u001b[39many(axis\u001b[38;5;241m=\u001b[39magg_axis, bool_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\dtypes\\missing.py:429\u001b[0m, in \u001b[0;36mnotna\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnotna\u001b[39m(obj: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_] \u001b[38;5;241m|\u001b[39m NDFrame:\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    Detect non-missing values for an array-like object.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;124;03m    Name: 1, dtype: bool\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\dtypes\\missing.py:183\u001b[0m, in \u001b[0;36misna\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misna\u001b[39m(obj: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_] \u001b[38;5;241m|\u001b[39m NDFrame:\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    Detect missing values for an array-like object.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    Name: 1, dtype: bool\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_isna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\dtypes\\missing.py:226\u001b[0m, in \u001b[0;36m_isna\u001b[1;34m(obj, inf_as_na)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, ABCDataFrame):\n\u001b[1;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _isna_array(np\u001b[38;5;241m.\u001b[39masarray(obj, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m), inf_as_na\u001b[38;5;241m=\u001b[39minf_as_na)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\frame.py:6227\u001b[0m, in \u001b[0;36mDataFrame.isna\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   6225\u001b[0m \u001b[38;5;129m@doc\u001b[39m(NDFrame\u001b[38;5;241m.\u001b[39misna, klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   6226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misna\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m-> 6227\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43misna\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   6228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misna\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\internals\\base.py:142\u001b[0m, in \u001b[0;36mDataManager.isna\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misna\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, func) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapply\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    355\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:329\u001b[0m, in \u001b[0;36mBlock.apply\u001b[1;34m(self, func, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[0;32m    325\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\dtypes\\missing.py:183\u001b[0m, in \u001b[0;36misna\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misna\u001b[39m(obj: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_] \u001b[38;5;241m|\u001b[39m NDFrame:\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    Detect missing values for an array-like object.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    Name: 1, dtype: bool\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_isna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\dtypes\\missing.py:212\u001b[0m, in \u001b[0;36m_isna\u001b[1;34m(obj, inf_as_na)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (np\u001b[38;5;241m.\u001b[39mndarray, ABCExtensionArray)):\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_isna_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_as_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minf_as_na\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, ABCIndex):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# Try to use cached isna, which also short-circuits for integer dtypes\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m#  and avoids materializing RangeIndex._values\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_can_hold_na:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\dtypes\\missing.py:302\u001b[0m, in \u001b[0;36m_isna_array\u001b[1;34m(values, inf_as_na)\u001b[0m\n\u001b[0;32m    300\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misfinite(values)\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 302\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Getting Data Stats\n",
    "\n",
    "print(\"----------------------Reading in Data----------------------------------------------------------------------\")\n",
    "\n",
    "#load in data\n",
    "data = pd.read_csv(\"sdss_100k_galaxy_form_burst.csv\")\n",
    "\n",
    "print(\"\\n------------------------------------- First Two Instances---------------------------------------------------\")\n",
    "print(data.head(2))\n",
    "\n",
    "# Get a summary of the DataFrame, including data types and non-null values\n",
    "print(\"\\n----------------------------------Dataset Information ---------------------------------------------------------------\")\n",
    "data.info()\n",
    "\n",
    "# Get descriptive statistics for numerical columns\n",
    "print(\"\\n----------------------------------------------- Descriptive Statistics ----------------------------------------------\")\n",
    "print(data.describe())\n",
    "\n",
    "# the dataset is enough to just drop values and duplicates\n",
    "data = data.dropna()\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa1c31-07e5-4c51-ac91-a86a0c6ea6ad",
   "metadata": {},
   "source": [
    "### Getting Unique Classes and Splitting Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe87f2-848f-4fd2-ba86-05d8a9e22351",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n----------------------------------------Unique Classes---------------------------------------------------------------\")\n",
    "num_classes_nunique = data['subclass'].nunique()\n",
    "print(f\"Number of classes : {num_classes_nunique}\")\n",
    "\n",
    "# Absolute counts\n",
    "print(\"\\nValue counts :\")\n",
    "print(data['subclass'].value_counts())\n",
    "\n",
    "print(\"\\nValue counts :\")\n",
    "print((data['subclass'].value_counts(normalize=True) * 100).round(2))\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------------------Splitting Data-------------------------------------------------------------\")\n",
    "y = data['subclass']\n",
    "X = data.drop(['subclass','class'], axis=1) #they are all galaxies hence we drop class\n",
    "\n",
    "print(\"\\n----------------------------------------X Dataset-------------------------------------------------------------------\")\n",
    "print(X.head(2))\n",
    "\n",
    "print(\"\\n----------------------------------------Y Dataset-------------------------------------------------------------------\")\n",
    "print(y.head(2))\n",
    "\n",
    "#shuffling dataset for the split\n",
    "data = data.sample(frac=1, random_state=42)\n",
    "\n",
    "X_train , X_test , y_train , y_test  = train_test_split(X,y, test_size=0.2 , random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c172e3-8712-4d7c-ba4b-0f427615baca",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547b01a-067c-4ae9-b27b-df968df720b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------------------------------Outliers----------------------------------------------------------------\")\n",
    "Q1 = X_train.quantile(0.25) \n",
    "Q3 = X_train.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers_iqr = ((X_train < (Q1 - 1.5 * IQR)) | (X_train > (Q3 + 1.5 * IQR)))\n",
    "print(\"Number of outliers per column:\")\n",
    "print(outliers_iqr.sum())\n",
    "\n",
    "#using IsolationForest to handle outliers and replace the outliers with the mean of the nearest neighbour\n",
    "#IsolationForest is used to detect outliers in the training dataset\n",
    "iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "outliers = iso.fit_predict(X_train)\n",
    "outlier_preds = iso.predict(X_train)\n",
    "\n",
    "#Replacing outliers with the mean of the nearest inlier\n",
    "inlier_mask = outlier_preds ==1\n",
    "outlier_mask = outlier_preds == -1\n",
    "\n",
    "X_inliers = X_train[inlier_mask]\n",
    "X_outliers = X_train[outlier_mask]\n",
    "\n",
    "#Fit the nearest neighbor on inliers\n",
    "num_neighbors = NearestNeighbors(n_neighbors=5)\n",
    "num_neighbors.fit(X_inliers)\n",
    "\n",
    "# Find nearest neighbors for each outlier\n",
    "distances, indices = num_neighbors.kneighbors(X_outliers)\n",
    "\n",
    "#Locate the nearest neighbor for every outlier\n",
    "mean_neighbors = []\n",
    "for neighbors_idx in indices:\n",
    "    mean_point = X_inliers.iloc[neighbors_idx].mean(axis=0)\n",
    "    mean_neighbors.append(mean_point)\n",
    "\n",
    "mean_neighbors = pd.DataFrame(mean_neighbors, columns=X_train.columns, index=X_outliers.index)\n",
    "print(\"--------------------Outliers before Replacement-------------------------------------------\")\n",
    "print(mean_neighbors)\n",
    "print(\"------------------------Shape-----------------------------\")\n",
    "print(mean_neighbors.shape)\n",
    "#replacement of the outlier rows\n",
    "X_train.loc[outlier_mask] = mean_neighbors\n",
    "\n",
    "#Verify the replaced rows\n",
    "print(\"-------------------------Replaced Outlier Rows With Mean---------------------------------------\")\n",
    "print(X_train.loc[outlier_mask].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32ed51a-03dc-4491-ab6b-120079b8fa7f",
   "metadata": {},
   "source": [
    "### Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f8ca3-72b6-4a19-9864-4546d6bd7023",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------------------------------Normalizing Data--------------------------------------\")\n",
    "#normalizing the data with MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ebc1f8-fe91-480a-86ed-ff4f72822f69",
   "metadata": {},
   "source": [
    "### Handling Data Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9fce97-7efb-4f72-b03b-b21ec07ea270",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------------Data Imbalance Handling-----------------------------------------\")\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Before SMOTE:\", Counter(y_train))\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train , y_train = smote.fit_resample(X_train, y_train) \n",
    "print(\"After SMOTE :\", Counter(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4cbd9e-55f7-486a-8845-cacd44d8c702",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fa017-a04e-4065-8c31-beb8d9b52c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding subclasses\n",
    "print(\"-----------------------------------Encoding----------------------------------------------------------------------\")\n",
    "#encoding subclasses to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Verify encoded classes\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "print(\"Encoded values:\", np.unique(y_train_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9900741a-6f42-43a9-b623-ee3166ab120a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50103205-3f7f-42e8-b04d-a748a2737e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_preds_test = iso.predict(X_test) #not refitting the dataset to avoid data leakage\n",
    "print(outlier_preds_test)\n",
    "\n",
    "#test Outliers replaced with pre-fitted nearest neighbors\n",
    "test_outliermask = outlier_preds_test == -1\n",
    "X_test_outliers = X_test[test_outliermask]\n",
    "distances, indices = num_neighbors.kneighbors(X_test_outliers)\n",
    "\n",
    "#replace with nearest neighbor of the training dataset\n",
    "#Locate the nearest neighbor for every outlier\n",
    "mean_neighbors_test = []\n",
    "for neighbors_idx in indices:\n",
    "    mean_point = X_inliers.iloc[neighbors_idx].mean(axis=0)\n",
    "    mean_neighbors_test.append(mean_point)\n",
    "\n",
    "neighbors_test = pd.DataFrame(mean_neighbors_test, columns=X_test.columns, index=X_test_outliers.index)\n",
    "#replacement of the outlier rows\n",
    "X_test.loc[test_outliermask] = neighbors_test\n",
    "#Normalize Data\n",
    "X_test = scaler.transform(X_test)\n",
    "#Feature selection used on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ee86a-5568-4753-9271-ce85c71f9320",
   "metadata": {},
   "source": [
    "## Sign-Based Adaptive Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b68d75-aa5a-4223-b5f4-0b634c99a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignBasedAdaptiveLR(tf.keras.optimizers.Adam): \n",
    "    def __init__(self, \n",
    "                 learning_rate=0.001,\n",
    "                 increase_factor=1.05,  # Factor to increase LR when sign is consistent\n",
    "                 decrease_factor=0.95,  # Factor to decrease LR when sign changes\n",
    "                 min_lr=1e-6,\n",
    "                 max_lr=0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(learning_rate=learning_rate, **kwargs)\n",
    "        self.increase_factor = increase_factor\n",
    "        self.decrease_factor = decrease_factor\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.previous_gradients = {}\n",
    "        self.weight_learning_rates = {}\n",
    "    \n",
    "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "        \n",
    "        var_key = var.ref()\n",
    "        \n",
    "        # Initialize previous gradient and learning rate for this variable\n",
    "        if var_key not in self.previous_gradients:\n",
    "            self.previous_gradients[var_key] = tf.zeros_like(grad)\n",
    "            self.weight_learning_rates[var_key] = self.learning_rate\n",
    "        \n",
    "        # Get previous gradient and current learning rate for this weight\n",
    "        prev_grad = self.previous_gradients[var_key]\n",
    "        current_lr = self.weight_learning_rates[var_key]\n",
    "        \n",
    "        # Check if gradient sign has changed\n",
    "        # Sign is consistent if prev_grad * grad > 0 (same sign)\n",
    "        sign_consistent = tf.reduce_mean(tf.cast(prev_grad * grad > 0, tf.float32))\n",
    "        \n",
    "        # Adapt learning rate based on sign consistency\n",
    "        # If more than 50% of gradients have consistent signs, increase LR\n",
    "        # Otherwise, decrease LR\n",
    "        if sign_consistent > 0.5:\n",
    "            new_lr = tf.minimum(current_lr * self.increase_factor, self.max_lr)\n",
    "        else:\n",
    "            new_lr = tf.maximum(current_lr * self.decrease_factor, self.min_lr)\n",
    "        \n",
    "        # Update stored values\n",
    "        self.previous_gradients[var_key] = grad\n",
    "        self.weight_learning_rates[var_key] = new_lr\n",
    "        \n",
    "        # Apply the gradient update with adapted learning rate\n",
    "        return var.assign_sub(new_lr * grad)\n",
    "    \n",
    "    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n",
    "        \"\"\"Sparse gradient updates (for embeddings, etc.)\"\"\"\n",
    "        return self._resource_apply_dense(tf.convert_to_tensor(grad), var, apply_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b2dc99-846b-4c0f-a3b8-50a6c446f449",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Creation using Custom Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db8bba3-622b-4eec-8300-90a7389f3a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_opti(input_dim, hidden_units,learning_rate):\n",
    "    print(\"---------------------Training with Sign-Based Adaptive Learning Rate---------------------------------\")\n",
    "    model_adaptive = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(hidden_units, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "        \n",
    "    optimizer_adaptive = SignBasedAdaptiveLR(\n",
    "            learning_rate=learning_rate,\n",
    "            increase_factor=1.05,\n",
    "            decrease_factor=0.05\n",
    "        )\n",
    "        \n",
    "    model_adaptive.compile(\n",
    "            optimizer=optimizer_adaptive,\n",
    "            loss=BinaryCrossentropy(),\n",
    "            metrics=[BinaryAccuracy()]\n",
    "        )\n",
    "        \n",
    "    return model_adaptive\n",
    "\n",
    "\n",
    "def train_and_evaluate_model_adaptive(X_train, y_train, X_test, y_test, \n",
    "                          epochs=10, batch_size=32, learning_rate=0.001, hidden_units=64):\n",
    "\n",
    "    print(\"\\n-----------------------Creating Model---------------------------------------------------\\n\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model_opti(learning_rate=learning_rate, hidden_units=hidden_units,input_dim=X_train.shape[1])\n",
    "\n",
    "    print(\"\\n--------------------------------Training Model-------------------------------------------\\n\")\n",
    "    # Train model\n",
    "    history_adaptive = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"\\n-----------------------------Model Architecture-----------------------------------------------\\n\")\n",
    "    model.summary()\n",
    "    \n",
    "    print(\"-------------------------Evaluate Model--------------------------------------------------\")\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(\n",
    "        X_test, y_test,\n",
    "        batch_size=batch_size, \n",
    "        verbose=1\n",
    "    )\n",
    "    print(f'\\nTest accuracy: {test_acc}')\n",
    "    print(f'Test loss: {test_loss}')\n",
    "    return model , history_adaptive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a7b3a-183f-4ade-bcf2-fdf1ada30512",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Creation - Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc88ed-be76-41c5-8a20-8f470d124efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim, hidden_units,learning_rate):\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        layers.Input(input_dim), \n",
    "        layers.Dense(hidden_units, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "    \n",
    "    # Use Adam optimizer with specified learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # Use BinaryCrossentropy for binary classification\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=BinaryCrossentropy(),\n",
    "        metrics=[BinaryAccuracy()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, \n",
    "                          epochs=10, batch_size=32, learning_rate=0.001, hidden_units=64):\n",
    "\n",
    "    print(\"\\n-----------------------Creating Model---------------------------------------------------\\n\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(learning_rate=learning_rate, hidden_units=hidden_units,input_dim=X_train.shape[1])\n",
    "\n",
    "    print(\"\\n--------------------------------Training Model-------------------------------------------\\n\")\n",
    "    # Train model\n",
    "    training = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"\\n-----------------------------Model Architecture-----------------------------------------------\\n\")\n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\n------------------------------------Evaluate Model-----------------------------------------\\n\")\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(\n",
    "        X_test, y_test,\n",
    "        batch_size=batch_size, \n",
    "        verbose=1\n",
    "    )\n",
    "    print(f'\\nTest accuracy: {test_acc}')\n",
    "    print(f'Test loss: {test_loss}')\n",
    "    \n",
    "    return model, training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a51dd-9568-4d70-bad4-12ced085c75c",
   "metadata": {},
   "source": [
    "## Adaptive Model and using Grid Search for hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ca513b-bbc4-4a1a-be8a-1865c892c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n---------------------------------Training Model with Hyperparameter Optimization----------------------------------\\n\")\n",
    "\n",
    "# Create KerasClassifier wrapper for GridSearchCV\n",
    "keras_classifier = KerasClassifier(\n",
    "    model=create_model_opti,\n",
    "    model__input_dim=X_train.shape[1],\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "#the parameter grid for which the search will occur\n",
    "param_grid = {\n",
    "    'model__hidden_units': [32, 64, 128],\n",
    "    'model__learning_rate': [0.0001, 0.001, 0.01],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "print(\"Starting Grid Search...\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=keras_classifier, \n",
    "    param_grid=param_grid, \n",
    "    cv=3,  # Reduced CV folds for faster training\n",
    "    scoring='accuracy',\n",
    "    verbose=2,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train_encoded)\n",
    "\n",
    "print(\"\\n-----------------------------------Grid Search Results---------------------------------------------------\\n\")\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "print(\"\\n---------------------------------------Heatmap for Grid Search-------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# Extract results from grid search\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Get unique values for the two hyperparameters we're focusing on\n",
    "learning_rates = param_grid['model__learning_rate']\n",
    "hidden_units = param_grid['model__hidden_units']\n",
    "\n",
    "# Create pivot table for heatmap (averaging across batch sizes)\n",
    "heatmap_data = results_df.groupby(['param_model__learning_rate', 'param_model__hidden_units'])['mean_test_score'].mean().reset_index()\n",
    "heatmap_pivot = heatmap_data.pivot(index='param_model__hidden_units', columns='param_model__learning_rate', values='mean_test_score')\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_pivot, annot=True, fmt='.4f', cmap='YlOrRd', cbar_kws={'label': 'Mean CV Accuracy'})\n",
    "plt.title('Grid Search Results: Learning Rate vs Hidden Units')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Number of Hidden Units')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results \n",
    "print(\"\\nDetailed Grid Search Results (Mean ± Std Dev):\")\n",
    "print(\"=\"*80)\n",
    "for i in range(len(results_df)):\n",
    "    params = results_df.loc[i, 'params']\n",
    "    mean_score = results_df.loc[i, 'mean_test_score']\n",
    "    std_score = results_df.loc[i, 'std_test_score']\n",
    "    print(f\"Params: {params}\")\n",
    "    print(f\"  Score: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Train final model with best parameters\n",
    "print(\"\\n------------------------Training Final Model with Best Parameters------------------------------------------\")\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "        \n",
    "final_model, training_history = train_and_evaluate_model_adaptive(\n",
    "    X_train, y_train_encoded, \n",
    "    X_test, y_test_encoded,\n",
    "    epochs=15,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    learning_rate=best_params['model__learning_rate'],\n",
    "    hidden_units=best_params['model__hidden_units']\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cf8bbd-f0ff-45bb-840c-fbf7b4cf08c9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb9b2d4-12b9-4b15-9c62-ec086c8b2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n---------------------------Final Model Evaluation--------------------------------------------------\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = final_model.predict(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "precision = precision_score(y_test_encoded, y_pred)\n",
    "recall = recall_score(y_test_encoded, y_pred)\n",
    "f1 = f1_score(y_test_encoded, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f76cae-f2ff-408e-a31c-e091df3df84e",
   "metadata": {},
   "source": [
    "## Visualization Adaptive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ecf586-ae62-4de5-a22e-95e17fc037d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"--------------------------------Generating Visualizations Adaptive Model--------------------------------------------------------------\")\n",
    "\n",
    "# Plot training loss over epochs\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_history.history['loss'], label='Training Loss')\n",
    "plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_history.history['binary_accuracy'], label='Training Accuracy')\n",
    "plt.plot(training_history.history['val_binary_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d395145-eee0-4ac7-890b-af82e3a01f61",
   "metadata": {},
   "source": [
    "## Standard Model and using Grid Search for hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1208eb-a6de-4bca-b009-8cf7919d0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n---------------------------------Training Model with Hyperparameter Optimization----------------------------------\\n\")\n",
    "\n",
    "# Create KerasClassifier wrapper for GridSearchCV\n",
    "keras_classifier = KerasClassifier(\n",
    "    model=create_model,\n",
    "    model__input_dim=X_train.shape[1],\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "#the parameter grid for which the search will occur\n",
    "# Note: model parameters need 'model__' prefix for KerasClassifier\n",
    "param_grid = {\n",
    "    'model__hidden_units': [32, 64, 128],\n",
    "    'model__learning_rate': [0.0001, 0.001, 0.01],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "print(\"Starting Grid Search...\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=keras_classifier, \n",
    "    param_grid=param_grid, \n",
    "    cv=3,  # Reduced CV folds for faster training\n",
    "    scoring='accuracy',\n",
    "    verbose=2,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train_encoded)\n",
    "\n",
    "print(\"\\n-----------------------------------Grid Search Results---------------------------------------------------\\n\")\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "print(\"\\n---------------------------------------Heatmap for Grid Search-------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# Extract results from grid search\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Get unique values for the two hyperparameters we're focusing on\n",
    "learning_rates = param_grid['model__learning_rate']\n",
    "hidden_units = param_grid['model__hidden_units']\n",
    "\n",
    "# Create pivot table for heatmap (averaging across batch sizes)\n",
    "heatmap_data = results_df.groupby(['param_model__learning_rate', 'param_model__hidden_units'])['mean_test_score'].mean().reset_index()\n",
    "heatmap_pivot = heatmap_data.pivot(index='param_model__hidden_units', columns='param_model__learning_rate', values='mean_test_score')\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_pivot, annot=True, fmt='.4f', cmap='YlOrRd', cbar_kws={'label': 'Mean CV Accuracy'})\n",
    "plt.title('Grid Search Results: Learning Rate vs Hidden Units')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Number of Hidden Units')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results with standard deviations\n",
    "print(\"\\nDetailed Grid Search Results (Mean ± Std Dev):\")\n",
    "print(\"=\"*80)\n",
    "for i in range(len(results_df)):\n",
    "    params = results_df.loc[i, 'params']\n",
    "    mean_score = results_df.loc[i, 'mean_test_score']\n",
    "    std_score = results_df.loc[i, 'std_test_score']\n",
    "    print(f\"Params: {params}\")\n",
    "    print(f\"  Score: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Train final model with best parameters\n",
    "print(\"\\n------------------------Training Final Model with Best Parameters------------------------------------------\")\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "final_model, training_history = train_and_evaluate_model(\n",
    "    X_train, y_train_encoded, \n",
    "    X_test, y_test_encoded,\n",
    "    epochs=15,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    learning_rate=best_params['model__learning_rate'],\n",
    "    hidden_units=best_params['model__hidden_units']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10effd4b-072b-4d1a-8eef-993c07da6d19",
   "metadata": {},
   "source": [
    "## Visualization Standard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e14fe7-a5a8-4c1b-b90b-df366344edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"--------------------------------Generating Visualizations Standard Model--------------------------------------------------------------\")\n",
    "\n",
    "# Plot training loss over epochs\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_history.history['loss'], label='Training Loss')\n",
    "plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_history.history['binary_accuracy'], label='Training Accuracy')\n",
    "plt.plot(training_history.history['val_binary_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0820f0-e5ee-479c-ba94-d8e5ca3bd151",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952980b4-a21d-40e0-9fc0-c0d19e867118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n---------------------------Final Model Evaluation--------------------------------------------------\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = final_model.predict(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "precision = precision_score(y_test_encoded, y_pred)\n",
    "recall = recall_score(y_test_encoded, y_pred)\n",
    "f1 = f1_score(y_test_encoded, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e61d3-3de5-4c02-ac8b-7f65b6c8e6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
